# bmad-assist configuration (optimized example)
# This is an optimized reference config with recommended settings.
# Copy to your project and customize provider settings as needed.

project_name: project-name
user_name: Tester
communication_language: English
document_output_language: English
user_skill_level: expert              # expert to minimize LLM verbosity

# External paths (optional) - store docs/artifacts outside project root
# paths:
#   project_knowledge: /shared/docs/my-project    # PRD, architecture, epics
#   output_folder: /data/bmad-output/my-project   # generated artifacts

# Per-phase timeout configuration (seconds), for larger projects increase timeouts
timeouts:
  default: 600                  # 10 m
  retries: 3                    # NEW: retry provider invoke 3x on timeout (None=skip, 0=infinite)
  create_story: 900             # 15 m
  validate_story_synthesis: 900 # 15 m
  dev_story: 3600               # 1 h
  code_review_synthesis: 1200   # 20 m
  qa_plan_generate: 1200        # 20 m
  qa_plan_execute: 3600         # 1 h (Playwright!)
  qa_remediate: 3600            # 1 h (LLM fixes + re-test)
  
providers:
  # Default provider used for single-LLM phases when no phase_models specified
  master:
    provider: claude
    model: opus
    # model_name: glm-4.7
    # settings: ~/.claude/glm.json

  # Helper model for simple and fast tasks
  helper:
    provider: claude
    model: haiku
    # model_name: glm-4.5
    # settings: ~/.claude/glm.json

  # Additional providers for multi-provider scenarios
  multi:
    # Gemini providers
    - provider: gemini
      model: gemini-3-pro-preview
    - provider: gemini
      model: gemini-3-flash-preview
    - provider: gemini
      model: gemini-2.5-flash
    - provider: gemini
      model: gemini-2.5-flash-lite
    # - provider: gemini
    #   model: gemini-2.5-flash-lite
    # - provider: kimi
    #   model: kimi-code/kimi-for-coding
    #   thinking: true

    # Claude subprocess providers
    - provider: claude
      model: sonnet
      model_name: glm-4.7
      settings: ~/.claude/glm.json
    - provider: claude
      model: sonnet
      model_name: glm-4.7
      settings: ~/.claude/glm.json

    # OpenCode providers (NEW)
    # - provider: opencode
    #   model: opencode/claude-sonnet-4       # Claude Sonnet 4 via OpenCode
    # - provider: opencode
    #   model: opencode/claude-opus-4-5       # Claude Opus 4.5 via OpenCode
    # - provider: opencode
    #   model: zai-coding-plan/glm-4.7        # GLM 4.7 via Z.ai
    # - provider: opencode
    #   model: xai/grok-4                     # Grok 4 via xAI
    # - provider: opencode
    #   model: opencode/gemini-3-flash        # Gemini 3 Flash via OpenCode
    # - provider: opencode
    #   model: opencode/gemini-3-pro          # Gemini 3 Pro via OpenCode

    # Kilo / Kilocode providers
    # - provider: kilo
    #   model: kilo/meta-llama/llama-3.3-70b-instruct:free    # Default model
    # - provider: kilo
    #   model: kilo/openai/gpt-oss-120b:free
    # - provider: kilo
    #   model: kilo/z-ai/glm-4.7:free



    # Amp providers - Sourcegraph's Claude wrapper
    # Note: Amp uses "modes" instead of model names
    # WARNING: Only "smart" mode works with bmad-assist (rush/free lack tool use)
    # - provider: amp
    #   model: smart                          # Claude Opus 4.5 (most capable)

    # Cursor Agent providers
    # Command: cursor-agent --print --model "<MODEL>" --force "<PROMPT>"
    # - provider: cursor-agent
    #   model: auto                # Default model
    # - provider: cursor-agent
    #   model: composer-1

    # GitHub Copilot providers
    # Command: copilot -p "<PROMPT>" --allow-all-tools --yolo --model "<MODEL>"
    # - provider: copilot
    #   model: claude-haiku-4.5                         # Default model


# Per-Phase Model Configuration
phase_models:
  # Single-LLM phases - object format
  create_story:
    provider: claude
    model: opus
  validate_story_synthesis:
    provider: claude
    model: sonnet
    model_name: glm-4.7
    settings: ~/.claude/glm.json
  dev_story:
    provider: claude
    model: opus
  code_review_synthesis:
    provider: claude
    model: opus
  retrospective:
    provider: claude
    model: sonnet
    model_name: glm-4.7
    settings: ~/.claude/glm.json

  # Multi-LLM phases - array format
  # When phase_models defines a multi-LLM phase, ONLY the listed providers are used.
  # Master is NOT auto-added - you have full control over the validator/reviewer list.
  validate_story:
    - provider: gemini
      model: gemini-3-pro-preview
    - provider: gemini
      model: gemini-3-flash-preview
    - provider: gemini
      model: gemini-2.5-flash
    - provider: gemini
      model: gemini-2.5-flash-lite
    # - provider: gemini
    #   model: gemini-2.5-flash-lite
    # - provider: kimi
    #   model: kimi-code/kimi-for-coding
    #   thinking: true
    # - provider: kimi
    #   model: kimi-code/kimi-for-coding
    #   thinking: true
    - provider: claude
      model: sonnet
      model_name: glm-4.7
      settings: ~/.claude/glm.json
    # - provider: claude
    #   model: sonnet
    #   model_name: glm-4.7
    #   settings: ~/.claude/glm.json
    # - provider: claude
    #   model: sonnet
    #   model_name: glm-4.7
    #   settings: ~/.claude/glm.json

  # Same for code_review - full control over reviewer list
  code_review:
    - provider: gemini
      model: gemini-3-pro-preview
    - provider: gemini
      model: gemini-3-flash-preview
    - provider: gemini
      model: gemini-2.5-flash
    - provider: gemini
      model: gemini-2.5-flash-lite
    # - provider: gemini
    #   model: gemini-2.5-flash-lite
    # - provider: kimi
    #   model: kimi-code/kimi-for-coding
    #   thinking: true
    # - provider: kimi
    #   model: kimi-code/kimi-for-coding
    #   thinking: true
    - provider: claude
      model: sonnet
      model_name: glm-4.7
      settings: ~/.claude/glm.json
    # - provider: claude
    #   model: sonnet
    #   model_name: glm-4.7
    #   settings: ~/.claude/glm.json
    # - provider: claude
    #   model: sonnet
    #   model_name: glm-4.7
    #   settings: ~/.claude/glm.json


# Compiler configuration
compiler:
  # patch_path: .bmad-assist/patches  # Custom patch files directory

  # Source files context collection for workflow prompts
  source_context:
    # Per-workflow token budgets (0-99 = disabled)
    budgets:
      create_story: 60000
      validate_story: 15000
      validate_story_synthesis: 15000
      dev_story: 15000
      code_review: 30000
      code_review_synthesis: 30000
      default: 30000             # Fallback for unlisted workflows

    # File prioritization scoring weights
    scoring:
      in_file_list: 50           # Bonus for files in story's File List
      in_git_diff: 50            # Bonus for files in git diff
      is_test_file: -10          # Penalty for test files
      is_config_file: -5         # Penalty for config files (.yaml, .json, etc.)
      change_lines_factor: 1     # Points per changed line
      change_lines_cap: 50       # Max points from change_lines

    # Content extraction settings
    extraction:
      adaptive_threshold: 0.25   # If file_tokens > (budget/files)*threshold → hunks only
      hunk_context_lines: 20     # Min context lines around changes
      hunk_context_scale: 0.3    # Context = max(hunk_context_lines, changes * scale)
      max_files: 15              # Hard cap on files (prevents budget dilution)

  # Strategic document loading for workflow prompts
  # Controls which docs (PRD, Architecture, UX, project-context) are included
  strategic_context:
    budget: 15000                 # Total token cap for strategic docs (0 = disabled)

    # Default settings for all workflows
    defaults:
      include: [project-context, architecture] # Doc types to include (order matters)
      main_only: true            # For sharded docs: load only index.md

    # Per-workflow overrides (null fields inherit from defaults)
    create_story:
      include: [project-context, architecture, ux, prd]
    validate_story:
      include: [project-context, architecture, ux, prd]
    dev_story:
      include: [project-context, architecture, ux, prd]
    code_review:
      include: [project-context, architecture, ux]


benchmarking:
  enabled: true

# Loop configuration - full TEA Enterprise integration
loop:
  epic_setup:
    - tea_framework           # Initialize Playwright/Cypress (once per epic)
    - tea_ci                  # Scaffold CI pipeline
    - tea_test_design         # Generate system/epic test design
  story:
    - create_story
    - validate_story
    - validate_story_synthesis
    - atdd                    # Generate ATDD checklist before implementation
    - dev_story
    - code_review
    - code_review_synthesis
    - test_review             # Review test quality after code review
  epic_teardown:
    - trace                   # Generate traceability matrix
    - tea_nfr_assess          # NFR assessment before release
    - retrospective
    - qa_plan_generate        # Generate E2E test plan
    - qa_plan_execute         # Execute E2E tests
    - qa_remediate            # Collect issues, auto-fix or escalate

# TEA (Test Architect Enterprise) configuration
testarch:
  enabled: false                  # Set to true to enable TEA functionality
  engagement_model: auto          # off | lite | solo | integrated | auto
  atdd_mode: auto                 # off | auto | on

  # TEA Context Loader - injects TEA artifacts into workflow prompts
  context:
    enabled: true
    budget: 10000                  # Total token budget for TEA artifacts
    max_tokens_per_artifact: 4000
    max_files_per_resolver: 10    # For ATDD (multiple checklist files)
    workflows:
      dev_story:
        include: [test-design, atdd]
      code_review:
        include: [test-design]
      code_review_synthesis:
        include: [test-review]
      retrospective:
        include: [trace]

# Deep Verify configuration - multi-method artifact verification
deep_verify:
  enabled: true                       # Master switch for Deep Verify module

  # Provider configuration (optional)
  # If not specified, uses global 'helper' provider from providers section
  # Uncomment to override with a different provider for Deep Verify methods
  # provider:
  #   provider: claude                # Provider name: claude, gemini, kimi
  #   model: haiku                    # Model identifier for LLM calls
  #   # model_name: custom-name       # Display name (optional, for logs)
  #   # settings: ~/.claude/custom.json  # Settings file path (optional)
  #   # thinking: false               # Enable thinking mode for supported providers (kimi)

  # Scoring thresholds for verdict determination
  # score > reject_threshold → REJECT
  # accept_threshold < score ≤ reject_threshold → UNCERTAIN
  # score ≤ accept_threshold → ACCEPT
  reject_threshold: 6.0
  uncertain_high: 6.0
  uncertain_low: -3.0
  accept_threshold: -3.0

  # Severity weights for finding score calculation
  critical_weight: 4.0                # Weight for CRITICAL findings
  error_weight: 2.0                   # Weight for ERROR findings
  warning_weight: 1.0                 # Weight for WARNING findings
  info_weight: 0.5                    # Weight for INFO findings

  # Bonus for domains with zero findings (negative = reduces score)
  clean_pass_bonus: -0.5

  # LLM infrastructure configuration
  llm_config:
    max_retries: 3                    # Retry attempts for failed LLM calls
    base_delay_seconds: 1.0           # Initial retry delay
    max_delay_seconds: 30.0           # Maximum retry delay cap
    tokens_per_minute_limit: 100000   # Rate limiting
    cost_tracking_enabled: true       # Track LLM costs
    log_all_calls: true               # Log all LLM calls with timing
    default_timeout_seconds: 90       # Per-method default timeout
    total_timeout_seconds: 180        # Total verification timeout

  # Resource limits to prevent OOM and system instability
  resource_limits:
    max_artifact_size_bytes: 204800   # 200KB max input size
    max_line_count: 5000              # Max lines in artifact
    max_findings_per_method: 50       # Findings limit per method
    max_total_findings: 200           # Total findings limit
    regex_timeout_seconds: 5.0        # Timeout for regex matching

  # Verification methods configuration
  # Always-run methods (foundational)
  method_153_pattern_match:           # Regex pattern matching for vulnerabilities
    enabled: true
    timeout_seconds: 90
  method_154_boundary_analysis:       # Edge cases, off-by-one, null checks
    enabled: true
    timeout_seconds: 90

  # Conditional methods (run based on detected domains)
  method_155_assumption_surfacing:    # Hidden assumptions, race conditions (CONCURRENCY, API)
    enabled: true
    timeout_seconds: 90
  method_157_temporal_consistency:    # Race conditions, ordering (MESSAGING, STORAGE)
    enabled: true
    timeout_seconds: 90
  method_201_adversarial_review:      # Attacks, exploits, malicious input (SECURITY, API)
    enabled: true
    timeout_seconds: 120
  method_203_domain_expert:           # LLM simulates domain expert (all domains)
    enabled: true
    timeout_seconds: 120
  method_204_integration_analysis:    # External service failures (API, MESSAGING)
    enabled: true
    timeout_seconds: 90
  method_205_worst_case:              # Worst-case scenario construction (CONCURRENCY, MESSAGING)
    enabled: true
    timeout_seconds: 90

security_agent:
  enabled: true
  max_findings: 20
  retries: 1          # Security agent only (None = no retry)

notifications:
  enabled: true
  providers:
    - type: telegram
      bot_token: "${TELEGRAM_BOT_TOKEN}"
      chat_id: "${TELEGRAM_CHAT_ID}"
    - type: discord
      webhook_url: "${DISCORD_WEBHOOK_URL}"
  events:
    - story_started
    - story_completed
    - phase_completed
    - error_occurred
    - anomaly_detected
