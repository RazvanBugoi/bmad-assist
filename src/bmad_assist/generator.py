"""QA plan generator.

Story: Standalone QA Plan Workflow
Generates E2E test plans for completed epics using the qa-plan-generate workflow.
"""

from __future__ import annotations

import logging
import re
from collections.abc import Callable
from dataclasses import dataclass
from pathlib import Path

from bmad_assist.core.config import Config
from bmad_assist.core.types import EpicId
from bmad_assist.providers import get_provider
from bmad_assist.qa.checker import get_qa_plan_path, get_trace_path

logger = logging.getLogger(__name__)


@dataclass
class QAPlanResult:
    """Result of QA plan generation attempt.

    Attributes:
        success: Whether generation succeeded.
        epic_id: Epic that was processed.
        qa_plan_path: Path to generated QA plan (if success).
        trace_path: Path to traceability file (if generated).
        error: Error message if generation failed.
        skipped: True if skipped (already exists).

    """

    success: bool
    epic_id: EpicId
    qa_plan_path: Path | None = None
    trace_path: Path | None = None
    error: str | None = None
    skipped: bool = False

    @classmethod
    def skip(cls, epic_id: EpicId, qa_plan_path: Path) -> QAPlanResult:
        """Create a skipped result (plan already exists)."""
        return cls(
            success=True,
            epic_id=epic_id,
            qa_plan_path=qa_plan_path,
            skipped=True,
        )

    @classmethod
    def fail(cls, epic_id: EpicId, error: str) -> QAPlanResult:
        """Create a failed result."""
        return cls(
            success=False,
            epic_id=epic_id,
            error=error,
        )


def _get_epic_trace(
    config: Config,
    project_path: Path,
    epic_id: EpicId,
) -> Path | None:
    """Get epic-level traceability file if it exists.

    Epic trace is generated by testarch-trace workflow with gate_type=epic.
    This function only checks for existing trace - it does NOT generate one.

    Args:
        config: Configuration instance.
        project_path: Project root directory.
        epic_id: Epic identifier.

    Returns:
        Path to trace file if exists, None otherwise.

    """
    trace_path = get_trace_path(config, project_path, epic_id)

    if trace_path.exists():
        logger.debug("Epic trace found: %s", trace_path)
        return trace_path

    logger.debug("No epic trace for %s at %s", epic_id, trace_path)
    return None


def _build_qa_plan_prompt(
    config: Config,
    project_path: Path,
    epic_id: EpicId,
    trace_path: Path | None,
) -> str:
    """Build prompt for QA plan generation.

    Constructs a prompt that instructs the LLM to generate an E2E test plan
    for the given epic, using available context from project files.

    Args:
        config: Configuration instance.
        project_path: Project root directory.
        epic_id: Epic identifier.
        trace_path: Path to traceability file.

    Returns:
        Prompt string for LLM.

    """
    # Load epic file if exists
    epic_content = ""
    epic_patterns = [
        project_path / "docs" / "epics" / f"epic-{epic_id}.md",
        project_path / "docs" / "epics" / f"epic-{epic_id}-*.md",
        project_path / "_bmad-output" / "planning-artifacts" / f"*epic*{epic_id}*.md",
    ]
    for pattern in epic_patterns:
        if pattern.exists():
            epic_content = pattern.read_text(encoding="utf-8")
            break
        # Try glob
        matches = list(pattern.parent.glob(pattern.name)) if "*" in str(pattern) else []
        if matches:
            epic_content = matches[0].read_text(encoding="utf-8")
            break

    # Load trace file (if available)
    trace_content = ""
    if trace_path is not None and trace_path.exists():
        trace_content = trace_path.read_text(encoding="utf-8")

    # Load stories for this epic
    try:
        from bmad_assist.core.paths import get_paths

        stories_dir = get_paths().stories_dir
    except RuntimeError:
        # Fallback for standalone usage without initialized paths
        stories_dir = project_path / "_bmad-output" / "implementation-artifacts"
    stories_content = ""
    if stories_dir.exists():
        story_files = sorted(stories_dir.glob(f"{epic_id}-*.md"))
        for sf in story_files[:10]:  # Limit to first 10 stories
            stories_content += f"\n\n---\n# {sf.name}\n"
            stories_content += sf.read_text(encoding="utf-8")[:3000]  # Truncate

    prompt = f"""You are a Test Architect generating an E2E test plan for Epic {epic_id}.

## Task
Generate a comprehensive E2E test plan document in markdown format.

## Project Info
- Project root: `$PROJECT_ROOT` (use this variable, NEVER hardcode paths like /home/user/...)
- Virtual env: `$PROJECT_ROOT/.venv/bin/python`
- CLI command: `$PROJECT_ROOT/.venv/bin/bmad-assist`

## Epic Context
{epic_content[:5000] if epic_content else f"Epic {epic_id} (no epic file found)"}

## Traceability Data
{trace_content[:3000] if trace_content else "No traceability data available."}

## Stories Implemented
{stories_content[:8000] if stories_content else f"Stories for epic {epic_id} (files not found)"}

## Output Format
Generate the test plan with these sections:

1. **Test Categories Summary** - Count tests by category (A/B/C)
2. **Master Checklist** - All tests with ID, name, category, status columns
3. **Category A Tests** - CLI/API/File tests with commands and expected output
4. **Category B Tests** - UI/Playwright tests with steps and selectors (skip if no UI)
5. **Category C Tests** - Human verification checklists
6. **Traceability Matrix** - FR/AC to Test mapping
7. **Automation Recommendations** - data-testid suggestions

## Categories
- **A (Full Automation)**: CLI commands, REST API, file validation - 100% automatable
- **B (Playwright)**: UI tests requiring browser - automatable with Playwright
- **C (Human)**: External integrations, credentials, subjective assessment

## CRITICAL Rules for Test Scripts
1. **NEVER hardcode absolute paths** - Use `$PROJECT_ROOT` variable
2. **Always cleanup** - Add cleanup commands at end of each test block
3. **Use temp directories** - `mktemp -d` for isolation, cleanup after
4. **Include setup section** at document start:
```bash
# Setup - run once before tests
export PROJECT_ROOT="$(pwd)"
cd "$PROJECT_ROOT"
source .venv/bin/activate
```
5. **Each test should be idempotent** - can run multiple times
6. **Include expected exit codes** - `echo "Expected exit code: 0"`
7. **Use trap for cleanup** in complex tests:
```bash
cleanup() {{ rm -rf "$TMPDIR"; }}
trap cleanup EXIT
TMPDIR=$(mktemp -d)
```

## Test ID Format
Use `E{{epic_id}}-A##` for Category A, `E{{epic_id}}-B##` for B, `E{{epic_id}}-C##` for C.

Start your response with:
# E2E Test Plan - Epic {epic_id}

End with the marker:
<!-- QA_PLAN_END -->
"""
    return prompt


def _extract_qa_plan(output: str) -> str | None:
    """Extract QA plan content from LLM output.

    Args:
        output: Raw LLM output.

    Returns:
        Extracted QA plan content, or None if not found.

    """
    # Try to find content between markers
    if "<!-- QA_PLAN_END -->" in output:
        # Find start of plan
        start_match = re.search(r"#\s*E2E Test Plan", output)
        if start_match:
            end_idx = output.find("<!-- QA_PLAN_END -->")
            return output[start_match.start() : end_idx].strip()

    # Fallback: find markdown header
    start_match = re.search(r"#\s*E2E Test Plan", output)
    if start_match:
        return output[start_match.start() :].strip()

    # Last resort: return everything after first #
    if "#" in output:
        return output[output.find("#") :].strip()

    return None


def _run_qa_plan_workflow(
    config: Config,
    project_path: Path,
    epic_id: EpicId,
    qa_plan_path: Path,
    trace_path: Path | None,
) -> bool:
    """Execute qa-plan-generate workflow via LLM provider.

    Args:
        config: Configuration instance.
        project_path: Project root directory.
        epic_id: Epic identifier.
        qa_plan_path: Path where QA plan should be written.
        trace_path: Path to traceability file, or None if not available.

    Returns:
        True if workflow succeeded, False otherwise.

    """
    logger.info("Generating QA plan for epic %s via LLM...", epic_id)

    try:
        # Build prompt
        prompt = _build_qa_plan_prompt(config, project_path, epic_id, trace_path)
        logger.debug("QA plan prompt length: %d chars", len(prompt))

        # Get master provider
        provider = get_provider(config.providers.master.provider)

        # Invoke LLM
        logger.info("Invoking LLM for QA plan generation...")
        result = provider.invoke(
            prompt,
            model=config.providers.master.model,
            timeout=config.timeout,
            env_file=config.providers.master.env_file_path,
            env_overrides=dict(config.providers.master.env_overrides),
        )

        if result.exit_code != 0:
            logger.error("LLM invocation failed: %s", result.stderr)
            return False

        # Extract QA plan from output
        qa_content = _extract_qa_plan(result.stdout)
        if not qa_content:
            logger.warning("Could not extract QA plan from LLM output, using raw output")
            qa_content = result.stdout

        # Ensure directory exists and write file
        qa_plan_path.parent.mkdir(parents=True, exist_ok=True)
        qa_plan_path.write_text(qa_content, encoding="utf-8")
        logger.info("QA plan generated: %s", qa_plan_path)
        return True

    except Exception as e:
        logger.error("Failed to generate QA plan for epic %s: %s", epic_id, e)
        return False


def generate_qa_plan(
    config: Config,
    project_path: Path,
    epic_id: EpicId,
    *,
    force: bool = False,
) -> QAPlanResult:
    """Generate QA plan for a completed epic.

    This function orchestrates the full QA plan generation:
    1. Check if plan already exists (skip if so, unless force=True)
    2. Ensure traceability file exists (generate via testarch-trace if not)
    3. Run qa-plan-generate workflow
    4. Return result with paths to generated artifacts

    Args:
        config: Configuration instance.
        project_path: Project root directory.
        epic_id: Epic identifier to generate plan for.
        force: If True, regenerate even if plan exists.

    Returns:
        QAPlanResult with success status and artifact paths.

    Example:
        >>> result = generate_qa_plan(config, project_path, 16)
        >>> if result.success:
        ...     print(f"QA plan: {result.qa_plan_path}")

    """
    qa_plan_path = get_qa_plan_path(config, project_path, epic_id)

    # Check if already exists
    if qa_plan_path.exists() and not force:
        logger.info("QA plan already exists for epic %s, skipping", epic_id)
        return QAPlanResult.skip(epic_id, qa_plan_path)

    # Check for existing epic trace (optional - QA plan can work without)
    trace_path = _get_epic_trace(config, project_path, epic_id)
    if trace_path is None:
        logger.info("No epic trace for %s, generating QA plan without trace data", epic_id)

    # Run QA plan workflow
    if not _run_qa_plan_workflow(config, project_path, epic_id, qa_plan_path, trace_path):
        return QAPlanResult.fail(
            epic_id,
            f"Failed to generate QA plan for epic {epic_id}",
        )

    return QAPlanResult(
        success=True,
        epic_id=epic_id,
        qa_plan_path=qa_plan_path,
        trace_path=trace_path,
    )


def generate_missing_qa_plans(
    config: Config,
    project_path: Path,
    missing_epics: list[EpicId],
    *,
    non_interactive: bool = False,
    prompt_fn: Callable[[str, bool], bool] | None = None,
) -> list[QAPlanResult]:
    """Generate QA plans for multiple epics.

    In interactive mode, prompts user for each epic.
    In non-interactive mode, generates all automatically.

    Args:
        config: Configuration instance.
        project_path: Project root directory.
        missing_epics: List of epic IDs without QA plans.
        non_interactive: If True, skip prompts and generate all.
        prompt_fn: Optional function to prompt user (for testing).
            Signature: prompt_fn(message: str, default: bool) -> bool

    Returns:
        List of QAPlanResult for each epic processed.

    """
    results: list[QAPlanResult] = []

    for epic_id in missing_epics:
        if non_interactive:
            # Auto-generate in non-interactive mode
            logger.info("Auto-generating QA plan for epic %s", epic_id)
            result = generate_qa_plan(config, project_path, epic_id)
            results.append(result)
        else:
            # Interactive mode - use prompt_fn or default to typer
            should_generate = True

            if prompt_fn is not None:
                should_generate = prompt_fn(
                    f"Epic {epic_id} has no QA plan. Generate?",
                    True,  # default value
                )
            else:
                # Use typer for interactive prompt
                try:
                    import typer

                    should_generate = typer.confirm(
                        f"Epic {epic_id} has no QA plan. Generate?",
                        default=True,
                    )
                except Exception:
                    # Fallback to auto-generate if typer unavailable
                    should_generate = True

            if should_generate:
                result = generate_qa_plan(config, project_path, epic_id)
                results.append(result)
            else:
                logger.info("Skipped QA plan for epic %s (user declined)", epic_id)

    return results
